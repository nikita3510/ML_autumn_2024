{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vQGqgYM9SUr"
      },
      "source": [
        "# Создание глубокой нейронной сети шаг за шагом\n",
        "\n",
        "Добро пожаловать! Ранее вы обучили двухслойную нейронную сеть (с одним скрытым слоем). На этой неделе вы построите глубокую нейронную сеть с любым количеством слоев!\n",
        "\n",
        "- В этом блокноте вы реализуете все функции, необходимые для построения глубокой нейронной сети.\n",
        "- В следующем задании вы будете использовать эти функции для построения глубокой нейронной сети для классификации изображений.\n",
        "\n",
        "**После этого задания вы сможете:**  \n",
        "- Использовать нелинейные функции, такие как ReLU, чтобы улучшить свою модель.\n",
        "- Построить более глубокую нейронную сеть (с более чем одним скрытым слоем)\n",
        "- Реализуете простой в использовании класс нейронной сети.\n",
        "\n",
        "**Обозначение**:  \n",
        "- Верхний индекс $[l]$ обозначает величину, связанную со слоем $l$.\n",
        "     - Пример: $a^{[L]}$ — это активация слоя $L$. $W^{[L]}$ и $b^{[L]}$ — параметры слоя $L$.\n",
        "- Надстрочный индекс $(i)$ обозначает величину, связанную с примером $i$.\n",
        "     - Пример: $x^{(i)}$ — это обучающий пример $i$.\n",
        "- Нижний индекс $i$ обозначает $i$-й элемент вектора.\n",
        "     - Пример: $a^{[l]}_i$ обозначает $i$-й запись активаций слоя $l$).\n",
        "\n",
        "Давайте начнем!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQewHMJl9SUt"
      },
      "source": [
        "## 1 - Пакеты\n",
        "\n",
        "Давайте сначала импортируем все пакеты, которые вам понадобятся во время этого задания.\n",
        "- [numpy](www.numpy.org) — основной пакет для научных вычислений на Python.\n",
        "- [matplotlib](http://matplotlib.org) — библиотека для построения графиков на Python.\n",
        "- dnn_utils предоставляет некоторые необходимые функции для этого блокнота.\n",
        "- testCases предоставляет несколько тестовых примеров для оценки правильности ваших функций.\n",
        "- np.random.seed(1) используется для обеспечения согласованности всех случайных вызовов функций. Это поможет нам оценить вашу работу. Пожалуйста, не меняйте seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nDffjnzK9SUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfc7078-7630-4ee6-fc2d-34c836c578af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from testCases_v4 import *\n",
        "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "lcwoC4Tr9SUv"
      },
      "source": [
        "## 2 – Краткое описание задания\n",
        "\n",
        "Чтобы построить свою нейронную сеть, вы будете реализовывать несколько «вспомогательных функций». Эти вспомогательные функции будут использоваться в следующем задании для построения двухслойной нейронной сети и нейронной сети из L слоев. Каждая небольшая вспомогательная функция, которую вы реализуете, будет иметь подробные инструкции, которые проведут вас через необходимые шаги. Вот план этого задания, вы будете:\n",
        "\n",
        "- Инициализируете параметры для двухслойной сети и $L$-слойной нейронной сети.\n",
        "- Реализуете модуль прямого распространения (показан фиолетовым цветом на рисунке ниже).\n",
        "      - Завершите ЛИНЕЙНУЮ часть шага прямого распространения слоя (в результате получим $Z^{[l]}$).\n",
        "      - мы предоставляем Вам функцию АКТИВАЦИИ (relu/sigmoid).\n",
        "      - Объедините два предыдущих шага в новую прямую функцию [LINEAR->ACTIVATION].\n",
        "      - Соедините функцию прямого распространения [LINEAR->RELU] L-1 раз (для слоев с 1 по L-1) и добавите [LINEAR->SIGMOID] в конце (для последнего слоя $L$). Это дает вам новую функцию L_model_forward.\n",
        "- Подсчитаете потери.\n",
        "- Реализуете модуль обратного распространения (обозначен красным на рисунке ниже).\n",
        "     - Завершите ЛИНЕЙНУЮ часть этапа обратного распространения слоя.\n",
        "     - Мы даем вам градиент функции АКТИВАЦИИ (relu_backward/sigmoid_backward)\n",
        "     - Объедините два предыдущих шага в новую обратную функцию [LINEAR->ACTIVATION].\n",
        "     - Соедините [LINEAR->RELU] в обратном проходе L-1 раз и добавите [LINEAR->SIGMOID] в новой функции L_model_backward.\n",
        "- Наконец сделаете один шаг обучения - обновите параметры.\n",
        "\n",
        "<img src=\"images/final\n",
        " Outline.png\" style=\"width:800px;height:500px;\">\n",
        "<caption><center><bold> Рис. 1 </bold></center></caption><br>\n",
        "\n",
        "\n",
        "**Обратите внимание**, что каждой прямой функции соответствует обратная функция. Вот почему на каждом этапе вашего модуля пересылки вы будете сохранять некоторые значения в кеше. Кэшированные значения полезны для вычисления градиентов. Затем в модуле обратного распространения ошибки вы будете использовать кеш для расчета градиентов. Это задание покажет вам, как именно выполнить каждый из этих шагов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "64X1DMhx9SUw"
      },
      "source": [
        "## 3 - Инициализация\n",
        "\n",
        "Вы напишете две вспомогательные функции, которые будут инициализировать параметры вашей модели. Первая функция будет использоваться для инициализации параметров двухслойной модели. Вторая обобщит  процесс инициализации на $L$-слоёв .\n",
        "\n",
        "### 3.1 - 2-слойная нейронная сеть\n",
        "\n",
        "**Упражнение**. Создайте и инициализируйте параметры двухслойной нейронной сети.\n",
        "\n",
        "**Инструкции**:  \n",
        "- Структура модели: *LINEAR -> RELU -> LINEAR -> SIGMOID*.\n",
        "- Используйте случайную инициализацию весовых матриц. Используйте `np.random.randn(shape)*0.01` с правильными размерами.\n",
        "- Используйте нулевую инициализацию для смещений. Используйте `np.zeros(shape)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pxmFwKyi9SUw"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    np.random.seed(1)\n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    assert(W1.shape == (n_h, n_x))\n",
        "    assert(b1.shape == (n_h, 1))\n",
        "    assert(W2.shape == (n_y, n_h))\n",
        "    assert(b2.shape == (n_y, 1))\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmZGpFfg9SUx",
        "outputId": "0dcdd762-1876-4af9-bc0d-66c55b695f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
            " [-0.01072969  0.00865408 -0.02301539]]\n",
            "b1 = [[0.]\n",
            " [0.]]\n",
            "W2 = [[ 0.01744812 -0.00761207]]\n",
            "b2 = [[0.]]\n"
          ]
        }
      ],
      "source": [
        "parameters = initialize_parameters(3,2,1)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNc5XHpD9SUx"
      },
      "source": [
        "**Ожидаемый вывод**:\n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> **W1** </td>\n",
        "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
        " [-0.01072969  0.00865408 -0.02301539]] </td>\n",
        "  </tr>\n",
        "\n",
        "  <tr>\n",
        "    <td> **b1**</td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**W2**</td>\n",
        "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td> **b2** </td>\n",
        "    <td> [[ 0.]] </td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "4KjAECZ09SUy"
      },
      "source": [
        "### 3.2 - L-слойная нейронная сеть\n",
        "\n",
        "Инициализация более глубокой L-слойной нейронной сети сложнее, поскольку здесь гораздо больше весовых матриц и векторов смещения. При заполнении «initialize_parameters_deep» вы должны убедиться, что ваши размеры совпадают на всех слоях. Напомним, что $n^{[l]}$ — это количество единиц в слое $l$. Например, если размер входных данных $X$ равен $(12288, 209)$ (с $m=209$ примерами), то:\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "    <tr>\n",
        "        <td>  </td>\n",
        "        <td> **Shape of W** </td>\n",
        "        <td> **Shape of b**  </td>\n",
        "        <td> **Activation** </td>\n",
        "        <td> **Shape of Activation** </td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> **Layer 1** </td>\n",
        "        <td> $(n^{[1]},12288)$ </td>\n",
        "        <td> $(n^{[1]},1)$ </td>\n",
        "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td>\n",
        "        <td> $(n^{[1]},209)$ </td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> **Layer 2** </td>\n",
        "        <td> $(n^{[2]}, n^{[1]})$  </td>\n",
        "        <td> $(n^{[2]},1)$ </td>\n",
        "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td>\n",
        "        <td> $(n^{[2]}, 209)$ </td>\n",
        "    <tr>\n",
        "       <tr>\n",
        "        <td> $\\vdots$ </td>\n",
        "        <td> $\\vdots$  </td>\n",
        "        <td> $\\vdots$  </td>\n",
        "        <td> $\\vdots$</td>\n",
        "        <td> $\\vdots$  </td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> **Layer L-1** </td>\n",
        "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td>\n",
        "        <td> $(n^{[L-1]}, 1)$  </td>\n",
        "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td>\n",
        "        <td> $(n^{[L-1]}, 209)$ </td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> **Layer L** </td>\n",
        "        <td> $(n^{[L]}, n^{[L-1]})$ </td>\n",
        "        <td> $(n^{[L]}, 1)$ </td>\n",
        "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
        "        <td> $(n^{[L]}, 209)$  </td>\n",
        "    <tr>\n",
        "</table>\n",
        "\n",
        "Помните, что когда мы вычисляем $W X + b$ в Python, осуществляется broadcasting. Например, если:\n",
        "\n",
        "$$ W = \\begin{bmatrix}\n",
        "    j  & k  & l\\\\\n",
        "    m  & n & o \\\\\n",
        "    p  & q & r\n",
        "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
        "    a  & b  & c\\\\\n",
        "    d  & e & f \\\\\n",
        "    g  & h & i\n",
        "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
        "    s  \\\\\n",
        "    t  \\\\\n",
        "    u\n",
        "\\end{bmatrix}\\tag{2}$$\n",
        "\n",
        "Тогда $WX + b$ будет:\n",
        "\n",
        "$$ WX + b = \\begin{bmatrix}\n",
        "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
        "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
        "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
        "\\end{bmatrix}\\tag{3}  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuG4DQZ39SUz"
      },
      "source": [
        "**Упражнение**. Реализуйте инициализацию L-слойной нейронной сети.\n",
        "\n",
        "**Инструкции**:  \n",
        "- Структура модели: *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*. То есть она имеет $L-1$ слоев, использующих функцию активации ReLU, за которыми следует выходной слой с функцией активации сигмоидой.\n",
        "- Используйте случайную инициализацию весовых матриц. Используйте `np.random.randn(shape) * 0.01`.\n",
        "- Для смещений используйте инициализацию нулями. Используйте `np.zeros(shape)`.\n",
        "- Мы будем хранить $n^{[l]}$, количество нейронов в разных слоях, в переменной `layer_dims`. Например, `layer_dims` для «Модели классификации плоских данных» с прошлой недели был бы [2,4,1]: было два входа, один скрытый слой с 4 скрытыми единицами и выходной слой с 1 выходной единицей. Таким образом, форма `W1` была (4,2), `b1` была (4,1), `W2` была (1,4) и `b2` была (1,1). Теперь вы обобщите это на слои $L$!\n",
        "- Вот реализация для $L=1$ (однослойная нейронная сеть). Это должно вдохновить вас на реализацию общего случая (нейронная сеть L-слоя).\n",
        "```python\n",
        "    if L == 1:\n",
        "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
        "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PfWcHuxJ9SUz"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)  # количество слоев в сети\n",
        "\n",
        "    for l in range(1, L):\n",
        "        # Инициализация весов и смещений для каждого слоя\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01  # Весовые матрицы\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))  # Векторы смещений\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZA2-bBe9SUz",
        "outputId": "9e7c39d8-2320-430b-8db3-0b6d453fa926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
            " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
            " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
            " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
            " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
            " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
            "b2 = [[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ],
      "source": [
        "parameters = initialize_parameters_deep([5,4,3])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkuOSqz39SU0"
      },
      "source": [
        "**Ожидаемый выход**:\n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> **W1** </td>\n",
        "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
        " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
        " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
        " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**b1** </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**W2** </td>\n",
        "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
        " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
        " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**b2** </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LP2qMsT9SU0"
      },
      "source": [
        "## 4 - Модуль прямого распространения (forward propagation)\n",
        "\n",
        "### 4.1 - Линейный forward\n",
        "Теперь, когда вы инициализировали свои параметры, вы займетесь модулем прямого распространения (forward propagation). Вы начнете с реализации некоторых основных функций, которые вы будете использовать позже при реализации модели. Выполните три функции в следующем порядке:\n",
        "\n",
        "- LINEAR\n",
        "- LINEAR -> ACTIVATION, где ACTIVATION будет либо ReLU, либо Sigmoid.\n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (вся модель)\n",
        "\n",
        "Линейный модуль прямого распространения (векторизованный по всем примерам) вычисляет следующие уравнения:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "где $A^{[0]} = X$.\n",
        "\n",
        "**Упражнение**. Постройте линейную часть прямого распространения сигнала.\n",
        "\n",
        "**Напоминание**:  \n",
        "Математическое представление этого модуля: $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. Вам также может пригодиться `np.dot()`. Если ваши размеры не совпадают, может помочь вывод на печать `W.shape`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SLAoK-x39SU1"
      },
      "outputs": [],
      "source": [
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(W, A) + b\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    return Z, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQewivnO9SU1",
        "outputId": "3addc3b7-b982-476d-8c59-f9ed416f9200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z = [[ 3.26295337 -1.23429987]]\n"
          ]
        }
      ],
      "source": [
        "A, W, b = linear_forward_test_case()\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "print(\"Z = \" + str(Z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNtmkLn79SU1"
      },
      "source": [
        "**Ожидаемый вывод**:\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  \n",
        "  <tr>\n",
        "    <td> **Z** </td>\n",
        "    <td> [[ 3.26295337 -1.23429987]] </td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhddPrN29SU2"
      },
      "source": [
        "### 4.2 - Linear-Activation Forward\n",
        "\n",
        "В этом блокноте вы будете использовать две функции активации:\n",
        "\n",
        "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Мы предоставили вам функцию «Sigmoid». Эта функция возвращает **два** элемента: значение активации «a» и «кэш», содержащий «Z» (это то, что мы передадим в соответствующую обратную функцию). Чтобы использовать его, вы можете просто вызвать:\n",
        "``` python\n",
        "A, activation_cache = sigmoid(Z)\n",
        "```\n",
        "\n",
        "- **ReLU**: Математическая формула ReLu: $A = RELU(Z) = max(0, Z)$. Мы предоставили вам функцию relu. Эта функция возвращает **два** элемента: значение активации «A» и «кэш», содержащий «Z» (это то, что мы передадим в соответствующую обратную функцию). Чтобы использовать его, вы можете просто вызвать:\n",
        "``` python\n",
        "A, activation_cache = relu(Z)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC6iesv-9SU2"
      },
      "source": [
        "Для большего удобства вы сгруппируете две функции (Линейная и Активация) в одну функцию (ЛИНЕЙНАЯ->АКТИВАЦИЯ). Т.е. вы реализуете функцию, которая выполняет ЛИНЕЙНЫЙ шаг, за которым следует шаг АКТИВАЦИЯ.\n",
        "\n",
        "**Упражнение**: реализуйте метод прямого распространения *LINEAR->ACTIVATION*. Математическое соотношение: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]} )$, где активация \"g\" может быть sigmoid() или relu(). Используйте `linear_forward()` и правильную функцию активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "c-bJFg0K9SU3"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0, Z)\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    if activation == \"sigmoid\":\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    elif activation == \"relu\":\n",
        "        A, activation_cache = relu(Z)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid activation function. Use 'sigmoid' or 'relu'.\")\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvpozWR69SU3",
        "outputId": "023793af-8fc5-4543-8ed1-2bab7b31ff11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With sigmoid: A = [[0.96890023 0.11013289]]\n",
            "With ReLU: A = [[3.43896131 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "A_prev, W, b = linear_activation_forward_test_case()\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
        "print(\"With ReLU: A = \" + str(A))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6XkX9Z79SU3"
      },
      "source": [
        "**Ожидаемый вывод**:\n",
        "       \n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td> **With sigmoid: A ** </td>\n",
        "    <td > [[ 0.96890023  0.11013289]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **With ReLU: A ** </td>\n",
        "    <td > [[ 3.43896131  0.        ]]</td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMEgBvp9SU4"
      },
      "source": [
        "**Замечание**: В глубоком обучении вычисление \"[LINEAR->ACTIVATION]\" обычно считается одним слоем нейронной сети, а не двумя отдельными слоями."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcVPrczJ9SU4"
      },
      "source": [
        "### L-слойная модель\n",
        "\n",
        "Для еще большего удобства при реализации нейронной сети с $L$-слоями вам понадобится функция, которая повторяет предыдущую (`linear_activation_forward` с RELU) $L-1$ раз, а затем одну `linear_activation_forward` с SIGMOID.\n",
        "\n",
        "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
        "<caption><center><bold>Рисунок 2</bold>: *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
        "\n",
        "**Упражнение**. Реализуйте функцию прямого распространения описанной выше модели.\n",
        "\n",
        "**Инструкция**: В приведенном ниже коде переменная `AL` будет обозначать $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A ^{[L-1]} + b^{[L]})$. (Иногда это еще называют $\\hat{Y}$.)\n",
        "\n",
        "**Советы**:\n",
        "- Используйте функции, которые вы написали ранее\n",
        "- Используйте цикл for для репликации [LINEAR->RELU] (L-1) раз.\n",
        "- Не забывайте следить за кешами в списке «кеш». Чтобы добавить новое значение `c` в `list`, вы можете использовать `list.append(c)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BvbL7cgB9SU5"
      },
      "outputs": [],
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation='relu')\n",
        "        caches.append(cache)  # Сохранение кэша\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n",
        "    caches.append(cache)\n",
        "    assert(AL.shape == (1, X.shape[1]))\n",
        "\n",
        "    return AL, caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_xZh9Yv9SU5",
        "outputId": "7bf4444a-7055-45a3-f2d3-2aff3ce33ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
            "Length of caches list = 3\n"
          ]
        }
      ],
      "source": [
        "X, parameters = L_model_forward_test_case_2hidden()\n",
        "AL, caches = L_model_forward(X, parameters)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of caches list = \" + str(len(caches)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxV09eY69SU6"
      },
      "source": [
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td> **AL** </td>\n",
        "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **Length of caches list ** </td>\n",
        "    <td > 3 </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpFX6O7-9SU6"
      },
      "source": [
        "Большой! Теперь у вас есть полная функция прямого распространения, которая принимает входные данные X и выводит вектор-строку $A^{[L]}$, содержащий ваши прогнозы. Она также записывает все промежуточные значения в «кеши». Используя $A^{[L]}$, вы можете вычислить ошибку (лосс) своих прогнозов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEARjyvm9SU6"
      },
      "source": [
        "## 5 - Функция ошибок\n",
        "\n",
        "Теперь вы реализуете прямое и обратное распространение. Вам необходимо вычислить функцию ошибки, чтобы проверить, действительно ли ваша модель обучается.\n",
        "\n",
        "**Упражнение**. Вычислите лосс-функцию кросс-энтропии $J$ по следующей формуле:\n",
        "\n",
        "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fI5EGVeM9SU6"
      },
      "outputs": [],
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = - (1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDyWccDI9SU6",
        "outputId": "d331ecf5-555e-4f41-d95b-af4b3bfc1955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost = 0.41493159961539694\n"
          ]
        }
      ],
      "source": [
        "Y, AL = compute_cost_test_case()\n",
        "\n",
        "print(\"cost = \" + str(compute_cost(AL, Y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7P5DbdU9SU7"
      },
      "source": [
        "**Ожидаемый выход**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "    <td>**cost** </td>\n",
        "    <td> 0.41493159961539694</td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zbz-YVO9SU7"
      },
      "source": [
        "## 6 - Модуль обратного распространения (backward propagation)\n",
        "\n",
        "Как и в случае с прямым распространением сигнала (forward propagation), вы реализуете вспомогательные функции для обратного распространения ошибки (backward propagation). Помните, что обратное распространение используется для расчета градиента функции потерь относительно параметров.\n",
        "\n",
        "**Напоминание**:  \n",
        "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
        "<caption><center><bold>Рис. 3</bold>: Прямое и обратное распространение для *LINEAR->RELU->LINEAR->SIGMOID* <br> *Фиолетовые блоки представляют прямое распространение, а красные блоки — обратное распространение.* </center></caption>\n",
        "\n",
        "<!--\n",
        "Для тех из вас, кто является экспертом в матанализе (вам это не обязательно), можно использовать цепное правило исчисления для получения производной функции потерь $\\mathcal{L}$ по $z. ^{[1]}$ в двухслойной сети следующим образом:\n",
        "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
        "\n",
        "Чтобы вычислить градиент $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, вы используете предыдущее цепное правило и делаете $dW^{[1 ]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. Во время обратного распространения ошибки на каждом этапе вы умножаете текущий градиент на градиент, соответствующий конкретному слою, чтобы получить желаемый градиент.\n",
        "\n",
        "Аналогично, чтобы вычислить градиент $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, вы используете предыдущее цепное правило и делаете $db^{ [1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
        "\n",
        "Вот почему мы говорим об **обратном распространении**.\n",
        "!-->\n",
        "\n",
        "Теперь, как и в случае с прямым распространением, построим обратное распространение в три этапа:\n",
        "- LINEAR backward\n",
        "- LINEAR -> ACTIVATION backward, где ACTIVATION вычисляет производную активации ReLU или SIGMOID.\n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (вся модель)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8PQn229SU7"
      },
      "source": [
        "### 6.1 -  Linear backward\n",
        "\n",
        "Для слоя $l$ линейная часть равна: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (с последующей активацией).\n",
        "\n",
        "Предположим, вы уже вычислили производную $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Вы хотите получить $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
        "\n",
        "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
        "<caption><center> **Figure 4** </center></caption>\n",
        "\n",
        "Три выхода $(dW^{[l]}, db^{[l]}, dA^{[l]})$ вычисляются с использованием входных данных $dZ^{[l]}$. Вот формулы, которые вы можете использовать:\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHhkq-hM9SU7"
      },
      "source": [
        "**Упражнение**. Используйте три приведенные выше формулы, чтобы реализовать linear_backward()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9l-W0O3-9SU7"
      },
      "outputs": [],
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haPSNcIj9SU8",
        "outputId": "d4e0797d-a125-469c-ecfb-0f6ad941db2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dA_prev = [[ 0.51822968 -0.19517421]\n",
            " [-0.40506361  0.15255393]\n",
            " [ 2.37496825 -0.89445391]]\n",
            "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
            "db = [[0.50629448]]\n"
          ]
        }
      ],
      "source": [
        "# Set up some test inputs\n",
        "dZ, linear_cache = linear_backward_test_case()\n",
        "\n",
        "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF5e5r4S9SU8"
      },
      "source": [
        "**Ожидаемый вывод**:\n",
        "\n",
        "<table style=\"width:90%\">\n",
        "  <tr>\n",
        "    <td> **dA_prev** </td>\n",
        "    <td > [[ 0.51822968 -0.19517421]\n",
        "           [-0.40506361  0.15255393]\n",
        "           [ 2.37496825 -0.89445391]]\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **dW** </td>\n",
        "    <td > [[-0.10076895  1.40685096  1.64992505]] </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **db** </td>\n",
        "    <td> [[ 0.50629448]] </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AegTMWrQ9SU8"
      },
      "source": [
        "### 6.2 - Linear-Activation backward\n",
        "\n",
        "Далее вы создадите функцию, которая объединит две вспомогательные функции: **`linear_backward`** и обратный шаг для активации **`linear_activation_backward`**.\n",
        "\n",
        "Чтобы помочь вам реализовать «linear_activation_backward», мы предоставили две обратные функции:\n",
        "- **`sigmoid_backward`**: реализует обратное распространение для модуля SIGMOID. Вы можете вызвать его следующим образом:\n",
        "\n",
        "``` питон\n",
        "dZ = sigmoid_backward(dA, active_cache)\n",
        "```\n",
        "\n",
        "- **`relu_backward`**: реализует обратное распространение для модуля RELU. Вы можете вызвать его следующим образом:\n",
        "\n",
        "``` питон\n",
        "dZ = relu_backward(dA, active_cache)\n",
        "```\n",
        "\n",
        "Если $g(.)$ — функция активации,\n",
        "`sigmoid_backward` и `relu_backward` вычисляют $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.\n",
        "\n",
        "**Упражнение**. Реализуйте обратное распространение ошибки для слоя *LINEAR->ACTIVATION*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qOY6CbJ49SU9"
      },
      "outputs": [],
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx92a8WR9SU9",
        "outputId": "f296296c-6008-47c2-b745-1150b2751032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid:\n",
            "dA_prev = [[ 0.11017994  0.01105339]\n",
            " [ 0.09466817  0.00949723]\n",
            " [-0.05743092 -0.00576154]]\n",
            "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
            "db = [[-0.05729622]]\n",
            "\n",
            "relu:\n",
            "dA_prev = [[ 0.44090989  0.        ]\n",
            " [ 0.37883606  0.        ]\n",
            " [-0.2298228   0.        ]]\n",
            "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
            "db = [[-0.20837892]]\n"
          ]
        }
      ],
      "source": [
        "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
        "\n",
        "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
        "print (\"sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db) + \"\\n\")\n",
        "\n",
        "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
        "print (\"relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKb0fnBN9SU9"
      },
      "source": [
        "**Ожидаемый вывод для sigmoid:**\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td > dA_prev </td>\n",
        "           <td >[[ 0.11017994  0.01105339]\n",
        " [ 0.09466817  0.00949723]\n",
        " [-0.05743092 -0.00576154]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > dW </td>\n",
        "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > db </td>\n",
        "           <td > [[-0.05729622]] </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWQyGZm29SU9"
      },
      "source": [
        "**Ожидаемый вывод для relu:**\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td > dA_prev </td>\n",
        "           <td > [[ 0.44090989  0.        ]\n",
        " [ 0.37883606  0.        ]\n",
        " [-0.2298228   0.        ]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > dW </td>\n",
        "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > db </td>\n",
        "           <td > [[-0.20837892]] </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZPpdBOM9SU9"
      },
      "source": [
        "### 6.3 - L-модель назад\n",
        "\n",
        "Теперь вы реализуете обратную функцию для всей сети. Напомним, что когда вы реализовали функцию L_model_forward, на каждой итерации вы сохраняли кеш, содержащий (X,W,b и z). В модуле обратного распространения вы будете использовать эти переменные для вычисления градиентов. Поэтому в функции `L_model_backward` вы будете перебирать все скрытые слои в обратном порядке, начиная со слоя $L$. На каждом этапе вы будете использовать кэшированные значения слоя $l$ для обратного распространения данных по слою $l$. На рисунке 5 ниже показан обратный проход.\n",
        "\n",
        "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
        "<caption><center><bold>Рис. 5</bold>: Обратный проход </center></caption>\n",
        "\n",
        "**Инициализация обратного распространения ошибки**:  \n",
        "Чтобы выполнить обратное распространение, мы знаем, что выход сети будут следующим:\n",
        "$A^{[L]} = \\sigma(Z^{[L]})$. Таким образом, вашему коду необходимо вычислить `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
        "Для этого используйте формулу:\n",
        "``` питон\n",
        "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # производная стоимости по AL\n",
        "```\n",
        "\n",
        "Затем вы можете использовать этот градиент после активации «dAL», чтобы продолжить движение назад. Как видно на рисунке 5, теперь вы можете ввести dAL в реализованную вами обратную функцию LINEAR->SIGMOID (которая будет использовать кэшированные значения, сохраненные функцией L_model_forward). После этого вам придется использовать цикл for для перебора всех остальных слоев с помощью обратной функции LINEAR->RELU. Вы должны хранить каждый dA, dW и db в словаре градиентов. Для этого используйте эту формулу:\n",
        "\n",
        "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
        "\n",
        "Например, для $l=3$ это сохранит $dW^{[l]}$ в `grads[\"dW3\"]`.\n",
        "\n",
        "**Упражнение**: Реализуйте обратное распространение ошибки для модели *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Yo2DQ3a49SU-"
      },
      "outputs": [],
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape)\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation='sigmoid')\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation='relu')\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY3oT9R69SU-",
        "outputId": "972ece80-5abe-429b-aaf4-7059c3185c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
            "db1 = [[-0.22007063]\n",
            " [ 0.        ]\n",
            " [-0.02835349]]\n",
            "dA1 = [[ 0.12913162 -0.44014127]\n",
            " [-0.14175655  0.48317296]\n",
            " [ 0.01663708 -0.05670698]]\n"
          ]
        }
      ],
      "source": [
        "AL, Y_assess, caches = L_model_backward_test_case()\n",
        "grads = L_model_backward(AL, Y_assess, caches)\n",
        "print_grads(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijLs1pHz9SU-"
      },
      "source": [
        "**Ожидаемый вывод**\n",
        "\n",
        "<table style=\"width:60%\">\n",
        "  <tr>\n",
        "    <td > dW1 </td>\n",
        "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
        " [ 0.          0.          0.          0.        ]\n",
        " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > db1 </td>\n",
        "           <td > [[-0.22007063]\n",
        " [ 0.        ]\n",
        " [-0.02835349]] </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "  <td > dA1 </td>\n",
        "           <td > [[ 0.12913162 -0.44014127]\n",
        " [-0.14175655  0.48317296]\n",
        " [ 0.01663708 -0.05670698]] </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6MD6H6M9SU-"
      },
      "source": [
        "### 6.4 - Обновление параметров (обучение)\n",
        "\n",
        "В этом разделе вы обновите параметры модели, используя градиентный спуск:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "где $\\alpha$ — скорость обучения. После вычисления обновленных параметров сохраните их в словаре параметров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6DDfT2y9SU_"
      },
      "source": [
        "**Упражнение**. Реализуйте `update_parameters()`, чтобы обновить параметры с помощью градиентного спуска.\n",
        "\n",
        "**Инструкции**:  \n",
        "Обновите параметры, используя градиентный спуск для каждых $W^{[l]}$ и $b^{[l]}$ для $l = 1, 2, ..., L$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ucIRVVWV9SU_"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW_hyPwM9SU_",
        "outputId": "cd39742d-9161-411c-e76f-ec5156a5cc43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
            " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
            " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
            "b1 = [[-0.04659241]\n",
            " [-1.28888275]\n",
            " [ 0.53405496]]\n",
            "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
            "b2 = [[-0.84610769]]\n"
          ]
        }
      ],
      "source": [
        "parameters, grads = update_parameters_test_case()\n",
        "parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7C5ofRE9SU_"
      },
      "source": [
        "**Ожидаемый вывод**:\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "    <tr>\n",
        "    <td > W1 </td>\n",
        "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
        " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
        " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > b1 </td>\n",
        "           <td > [[-0.04659241]\n",
        " [-1.28888275]\n",
        " [ 0.53405496]] </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td > W2 </td>\n",
        "           <td > [[-0.55569196  0.0354055   1.32964895]]</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > b2 </td>\n",
        "           <td > [[-0.84610769]] </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj1AQhAC9SU_"
      },
      "source": [
        "## 7 - Заключение\n",
        "\n",
        "Поздравляем с реализацией всех функций, необходимых для построения глубокой нейронной сети!\n",
        "\n",
        "Мы знаем, что это было долгое задание, но в дальнейшем оно будет только лучше. Следующая часть задания проще.\n",
        "\n",
        "В следующем задании вы соедините все это вместе, чтобы построить две модели:\n",
        "- Двухслойная нейронная сеть\n",
        "- Нейронная сеть из L слоёв.\n",
        "\n",
        "Фактически вы будете использовать эти модели для классификации изображений кошек и не кошек!"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "c4HO0",
      "launcher_item_id": "lSYZM"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}